# Bob, Riker, Bill et Milo : Une √âquipe d'Agents IA pour une Recherche Web Intelligente avec Docker Model Runner et MCP Toolkit
> R√©aliser un pipeline d'Agents IA avec Docker Model Runner

## Introduction

Dans le pr√©c√©dent blog post j'expliquais comment impl√©menter une version du pattern AI Agent en Go avec Docker Model Runner et le Docker MCP Toolkit. Le blog post √©tait "plut√¥t long", et finalement, le plus souvent c'est par l'exemple que l'on apprend le mieux. Donc aujourd'hui prenons un cas concret. 

J'aimerais pouvoir faire des recherches web "am√©lior√©es". Par exemple, pouvoir demander `"What is Docker Compose? (Only 5 results)"` et obtenir √† la fin un document markdown structur√© avec les √©l√©ments essentiels et les sources utilis√©es.

Pour cela je vais avoir besoin de 4 agents:

- **Bob**, pour les recherches internet (`WebSearch`): obtenir une liste de sites correspondants au texte de ma recherche. **Bob** utilisera un serveur MCP pour effectuer les recherches.
- **Riker**, qui √† partir des r√©sultat de **Bob**, va g√©n√©rer un report au format JSON pour extraire notamment les URLs des r√©sultats de recherche (`ExtractDataFromResults`) et le fournir sous la forme d'un tableau de map `[]map[string]any`.
- **Bill**, va parcourir le tableau de map fourni par **Riker** et r√©cup√©rer le contenu de chacune des URLs (`FetchContent`) pour les restituer sous la forme d'un tableau de strings `[]string`. **Bill** utilisera un serveur MCP pour r√©cup√©rer les contenus.
- Et enfin, **Milo** qui √† partir de tout ce contenu va g√©n√©rer un r√©sum√© de ses recherches (`Summarize`).

Il est possible de le faire avec moins d'agents. Je pense m√™me que je pourrais le faire avec un seul (cf. [Hybrid Prompts with Docker Model Runner and the MCP Toolkit](https://k33g.hashnode.dev/hybrid-prompts-with-docker-model-runner-and-the-mcp-toolkit)). Mais la possibilit√© de d√©couper le travail et de le r√©partir sur plusieurs agents va me permettre de choisir des mod√®les plus petits et donc d'**acc√©l√©rer** certaines t√¢ches tout en **consommant moins de ressources** et aussi utiliser les mod√®les les plus adapt√©s √† telle ou telle t√¢che. 

```mermaid
flowchart TD
    A[User Query<br/>Example: What is Docker Compose<br/>Only 5 results] --> B[Bob Agent<br/>WebSearch]
    
    B --> B1[MCP Server<br/>Web Search]
    B1 --> B2[Search Results<br/>List of matching sites]
    B2 --> C[Riker Agent<br/>ExtractDataFromResults]
    
    C --> C1[Process Search Results]
    C1 --> C2[Generate JSON Report<br/>Extract URLs]
    C2 --> C3[Output: Array of URL mappings<br/>map string any array]
    
    C3 --> D[Bill Agent<br/>FetchContent]
    
    D --> D1[Parse URL Array]
    D1 --> D2[MCP Server<br/>Content Fetcher]
    D2 --> D3[Fetch Content<br/>from each URL]
    D3 --> D4[Output: Array of content strings<br/>string array]
    
    D4 --> E[Milo Agent<br/>Summarize]
    
    E --> E1[Process All Content]
    E1 --> E2[Generate Summary]
    E2 --> F[Final Output<br/>Structured Markdown Document<br/>with Essential Elements<br/>and Sources]
    
    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
    style E fill:#fff3e0
    style B1 fill:#f3e5f5
    style D2 fill:#f3e5f5
```

## Pr√©-requis

- Docker Desktop
- Installer l'extension **Docker MCP Toolkit**
- Installer les serveurs MCP suivants (via Docker MCP Toolkit):

- **[Brave Search](https://github.com/docker/labs-ai-tools-for-devs/blob/main/prompts/mcp/readmes/brave.md)** pour les recherches web. Vous aurez besoin d'une cl√© d'API. **Brave** propose un plan gratuit qui est largement suffisant poue ce type d'exemple: [https://brave.com/search/api/] ou le [serveur MCP **DuckDuckGo**](https://github.com/docker/labs-ai-tools-for-devs/blob/main/prompts/mcp/readmes/duckduckgo.md), mais attention il est soumis √† une limitation de nombre de requ√™tes.
- **[Fetch](https://github.com/docker/labs-ai-tools-for-devs/blob/main/prompts/mcp/readmes/fetch.md)** pour extraire le contenu de pages et le rendre au format markdown.

J'explique comment utiliser **Docker MCP Toolkit** dans ce blog post: [Boosting Docker Model Runner with Docker MCP Toolkit](https://k33g.hashnode.dev/boosting-docker-model-runner-with-docker-mcp-toolkit)

> **Remarque**: Je voulais r√©-utiliser le code de l'article pr√©c√©dent: [Minimalist Implementation of an AI Agent with Docker Model Runner and Docker MCP Toolkit](https://k33g.hashnode.dev/minimalist-implementation-of-an-ai-agent-with-docker-model-runner-and-docker-mcp-toolkit). J'ai donc transform√© ce code en une librairie: [https://github.com/sea-monkeys/robby](https://github.com/sea-monkeys/robby). J'insiste, c'est une librairie, pas un framework. Son objectif est juste de me faciliter la vie dans mon utilisation du SDK Go d'OpenAI et des serveurs MCP (et d'autres petits outils).

## Le programme principal

Le programme principal va ex√©cuter 4 fonctions successives:
- `WebSearch`: je fais une requ√™te web: `"What is Docker Compose? (Only 3 results)"`
- `ExtractDataFromResults`: j'en extraits les informations pour les structurer (r√©cup√©rer les URLs)
- `FetchContent`: pour chaque URL, je r√©cup√®re le contenu de la page associ√©e
- `Summarize`: je r√©sume et met en forme les contenus

Chaque fonction "contient" un AI Agent dont la responsabilit√© est d'ex√©cuter une t√¢che bien pr√©cise. Et son r√©sultat sera utilis√© par l'agent suivant, jusqu'√† l'obtention du r√©sultat attendu.

```golang
func main() {

	results, err := WebSearch("What is Docker Compose? (Only 3 results)")
	if err != nil {
		fmt.Println("Error:", err)
		return
	}

	data, err := ExtractDataFromResults(results)
	if err != nil {
		fmt.Println("Error extracting data:", err)
		return
	}

	content, err := FetchContent(data)

	if err != nil {
		fmt.Println("Error fetching content:", err)
		return
	}

	_, err = Summarize(`/no_think [Brief]
		Make a clear, and structured summaryt with the provided information.
		- Use markdown format.
		- Provide only verified refrences (URLs).
		- Stay focused and do not repeat the same information.	
		- Do not use any other external information.
		- Do not include the error messages in the report.
	`, content)

	if err != nil {
		fmt.Println("Error summarizing content:", err)
		return
	}

}
```
> J'ai pr√©vu d'utiliser **[ai/qwen3](https://hub.docker.com/r/ai/qwen3)** pour le LLM du dernier agent. L'utilisation de `/no_think` en d√©but de prompt, permet de d√©sactiver le mode raisonnement du mod√®le 

Donc maintenant il ne nous reste plus qu'√† impl√©menter les 4 fonctions.

## C'est parti, on commence par Bob pour les recherches Web

J'ai donc besoin d'un agent pour envoyer des requ√™tes √† un moteur de recherche. J'ai donc cod√© un agent (**"Bob"**) en utilisant [https://github.com/sea-monkeys/robby](https://github.com/sea-monkeys/robby), cette agent sera sollicit√© √† l'aide d'une fonction `WebSearch`. L'utilisation de la fonction apporte plus de lisibilit√© dans le code principal de la fonction `main`:

```golang

func WebSearch(query string) ([]string, error) {
	model := "ai/qwen2.5:0.5B-F16"
	Bob, _ := robby.NewAgent(
		robby.WithDMRClient(
			context.Background(),
			"http://model-runner.docker.internal/engines/llama.cpp/v1/",
		),
		robby.WithParams(
			openai.ChatCompletionNewParams{
				Model: model,
				Messages: []openai.ChatCompletionMessageParamUnion{
					openai.UserMessage(query),
				},
				Temperature:       openai.Opt(0.0),
				ParallelToolCalls: openai.Bool(true),
			},
		),
		robby.WithMCPClient(robby.WithDockerMCPToolkit()),
		robby.WithMCPTools([]string{"search"}), // `search` is a tool of DuckDuckGo
	)

	// Execute the tool calls == tool calls detection
	_, err := Bob.ToolsCompletion()
	if err != nil {
		return nil, err
	}

	toolCallsJSON, _ := Bob.ToolCallsToJSON()
	fmt.Println("Tool Calls:", toolCallsJSON)

	// Execute the tool calls and get the results
	results, _ := Bob.ExecuteMCPToolCalls()
	
	// Display the results
	for _, result := range results {
		fmt.Println(result)
	}
	fmt.Println("Web Search Results completed ‚úÖ")

	return results, nil
}
```

Cette fonction va cr√©er un agent "Bob" qui va "faire" du **function calling** en utilisant l'outil `search` du serveur MCP **DuckDuckGo**. Je d√©cris en d√©tail le fonctionnement du **function calling** dans ce post: [Function Calling with Docker Model Runner](https://k33g.hashnode.dev/function-calling-with-docker-model-runner).

L'ex√©cution d'outil d√©tect√© va √™tre la suivante:
```json
[
    {
        "function": {
            "arguments": {
                "max_results": 3,
                "query": "Docker Compose"
            },
            "name": "search"
        },
        "id": "ykRuqixONp9UQzIJzwvwV8jXeyMSQMyj"
    }
]
```

Le r√©sultat va √™tre le suivant:

```json
Found 3 search results:

1. Docker Compose | Docker Docs - Docker Documentation
   URL: https://docs.docker.com/compose/
   Summary: DockerComposeis a tool for defining and running multi-container applications. It is the key to unlocking a streamlined and efficient development and deployment experience.Composesimplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single YAML configuration file. ...

2. Docker Compose - GeeksforGeeks
   URL: https://www.geeksforgeeks.org/docker-compose/
   Summary: Learn how to useDockerComposeto run and manage multiple containers in a YAML-based file. See key concepts, configuration options, examples, and best practices forDockerCompose.

3. GitHub - docker/compose: Define and run multi-container applications ...
   URL: https://github.com/docker/compose
   Summary: DockerComposeis a tool for running multi-container applications onDockerdefined using theComposefile format. AComposefile is used to define how one or more containers that make up your application are configured. Once you have aComposefile, you can create and start your application with a single command:dockercomposeup.
```

**√Ä noter**: j'ai utilis√© un "tout petit" LLM: `ai/qwen2.5:0.5B-F16`, qui tout √† fait suffisant pour cette d√©tection d'outil. L'avantage c'est que l'agent chargera le LLM et ex√©cutera la compl√©tion **beaucoup plus rapidement**.

Nous allons ensuite avoir besoin d'extraire les URLs de ce contenu. Pour cela nous allons utiliser le principe du JSON Output Format qui permet aux LLMs qui le supportent de g√©n√©rer des r√©ponses au format JSON. J'ai √©crit un post sur ce principe: [Generating Structured Data with Docker Model Runner](https://k33g.hashnode.dev/generating-structured-data-with-docker-model-runner). Passons donc √† la cr√©ation du deuxi√®me agent


## Et maintenant, Riker pour l'extraction des donn√©es

Je code donc une deuxi√®me fonction nomm√©e `ExtractDataFromResults` qui ca cr√©er un deuxi√®me agent (**"Riker"**), qui va utiliser les donn√©es g√©n√©r√©es par **"Bob"** (via la fonction `WebSearch`) pour, √† partir d'un sch√©ma JSON, va g√©n√©rer une payload JSON et ensuite la transformer en `[]map[string]any`:

```golang
func ExtractDataFromResults(results []string) ([]map[string]any, error) {

	// NOTE: ai/qwen2.5:0.5B-F16 and ai/qwen2.5:1.5B-F16 are too small for this task
	model := "ai/qwen2.5:3B-F16"

	schema := map[string]any{
		"type": "array",
		"items": map[string]any{
			"type": "object",
			"properties": map[string]any{
				"title": map[string]any{
					"type":        "string",
					"description": "The first line of the section",
				},
				"url": map[string]any{
					"type": "string",
				},
				"summary": map[string]any{
					"type":        "string",
					"description": "A short summary of the section",
				},
			},
			"required": []string{"title", "url", "summary"},
		},
	}

	schemaParam := openai.ResponseFormatJSONSchemaJSONSchemaParam{
		Name:        "search_results",
		Description: openai.String("Notable information about search results"),
		Schema:      schema,
		Strict:      openai.Bool(true),
	}

	Riker, _ := robby.NewAgent(
		robby.WithDMRClient(
			context.Background(),
			"http://model-runner.docker.internal/engines/llama.cpp/v1/",
		),
		robby.WithParams(
			openai.ChatCompletionNewParams{
				Model: model,
				Messages: []openai.ChatCompletionMessageParamUnion{
					openai.SystemMessage(strings.Join(results, "\n")),
					openai.UserMessage("give me the list of the results."),
				},
				Temperature: openai.Opt(0.0),
				ResponseFormat: openai.ChatCompletionNewParamsResponseFormatUnion{
					OfJSONSchema: &openai.ResponseFormatJSONSchemaParam{
						JSONSchema: schemaParam,
					},
				},
			},
		),
	)
	jsonResults, err := Riker.ChatCompletion()
	if err != nil {
		return nil, err
	}

	fmt.Println("üìù JSON Results:\n", jsonResults)

	// Transform the json string into a map
	var jsonResultsMap []map[string]any
	err = json.Unmarshal([]byte(jsonResults), &jsonResultsMap)
	if err != nil {
		return nil, err
	}

	fmt.Println("Extracted Data from Results completed ‚úÖ")
	return jsonResultsMap, nil
}
```

Le r√©sultat va √™tre le suivant:

```json
[
    {
        "summary":"Docker Compose | Docker Docs - Docker Documentation",
        "title":"Docker Compose",
        "url":"https://docs.docker.com/compose/"
    },
    {
        "summary":"Docker Compose - GeeksforGeeks",
        "title":"Docker Compose",
        "url":"https://www.geeksforgeeks.org/docker-compose/"
    },
    {
        "summary":"GitHub - docker/compose: Define and run multi-container applications ...",
        "title":"Docker Compose",
        "url":"https://github.com/docker/compose"
    }
]
```

Nous avons donc maintenant une liste structur√©e avec 3 enregistrements, ce qui va nous permettre de demander √† l'agent suivant de construire un prompt √† partir de ces r√©sultats pour lui faire accomplir √† son tour du **function calling**.

**√Ä noter**: j'ai utilis√© un LLM plus gros: `ai/qwen2.5:3B-F16`, les LLM plus petits n'√©taient pas capables de g√©n√©rer une payload JSON avec toutes les donn√©es n√©cessaires.


## Le 3√©me agent, Bill pour le chargement des donn√©es

Je cr√©e donc une nouvelle fonction `FetchContent`, qui va cr√©er un nouvel agent (**"Bill"**) qui va √™tre en charge d'aller charger le contenu de chacunes des URLs fournie par l'agent pr√©c√©dent (**"Riker"**) via la fonction `ExtractDataFromResults`:

```golang
func FetchContent(data []map[string]any) ([]string, error) {

	model := "ai/qwen2.5:0.5B-F16"

	Bill, _ := robby.NewAgent(
		robby.WithDMRClient(
			context.Background(),
			"http://model-runner.docker.internal/engines/llama.cpp/v1/",
		),
		robby.WithParams(
			openai.ChatCompletionNewParams{
				Model:             model,
				Messages:          []openai.ChatCompletionMessageParamUnion{},
				Temperature:       openai.Opt(0.0),
				ParallelToolCalls: openai.Bool(true),
			},
		),
		robby.WithMCPClient(robby.WithDockerMCPToolkit()),
		robby.WithMCPTools([]string{"fetch"}),
	)

	prompt := ""
	for _, result := range data {
		prompt += fmt.Sprintf("Fetch this URL: %s\n", result["url"])
	}

	fmt.Println("üõ†Ô∏è Prompt for tool calls:")
	fmt.Println(prompt)

	Bill.Params.Messages = []openai.ChatCompletionMessageParamUnion{
		openai.UserMessage(prompt),
	}

	_, err := Bill.ToolsCompletion()
	if err != nil {
		return nil, fmt.Errorf("error in tools completion: %w", err)
	}

	toolCallsJSON, _ := Bill.ToolCallsToJSON()
	fmt.Println("Tool Calls:", toolCallsJSON)

	results, err := Bill.ExecuteMCPToolCalls()
	if err != nil {
		return nil, fmt.Errorf("error executing tool calls: %w", err)
	}

	fmt.Println("Fetched Content completed ‚úÖ")

	return results, nil
}
```

Cette fois ci, nous allons utiliser l'outil `fetch` du serveur MCP **Fetch**

On construit donc un nouveau prompt pour le nouvel agent √† partir des informations de **"Riker"**:
```raw
üõ†Ô∏è Prompt for tool calls:
Fetch this URL: https://docs.docker.com/compose/
Fetch this URL: https://www.geeksforgeeks.org/docker-compose/
Fetch this URL: https://github.com/docker/compose
```

Le LLM va ex√©cuter une "Tools Compl√©tion" et d√©tecter qu'il y a 3 tool calls dans le prompt:

```json
[
    {
        "function": {
            "arguments": {
                "raw": false,
                "start_index": 0,
                "url": "https://docs.docker.com/compose/"
            },
            "name": "fetch"
        },
        "id": "Jra4ZcOe7DY3mv1ZnTqZ6Fe0GfLzidrK"
    },
    {
        "function": {
            "arguments": {
                "raw": false,
                "start_index": 0,
                "url": "https://www.geeksforgeeks.org/docker-compose/"
            },
            "name": "fetch"
        },
        "id": "DcSrxiFvI5nJYtKPUe8b3IPwwJeTbAGN"
    },
    {
        "function": {
            "arguments": {
                "raw": false,
                "start_index": 0,
                "url": "https://github.com/docker/compose"
            },
            "name": "fetch"
        },
        "id": "9WcYM9d6BvCL7J1ZbcI0ci8ciz9XHbYW"
    }
]
```

Ensuite, l'agent va ex√©cuter √† partir de cette liste, les 3 appels successifs de l'outil `fetch` du serveur MCP **Fetch**.
Et enfin la fonction retournera les r√©sultats des fetchs sous la forme d'un tableau de strings (`[]string`).

Et nous pourrons finalement transmettre ces contenus √† un dernier agent pour la "mise en forme".

**√Ä noter**: j'ai pu √† nouveau utilis√© un "tout petit" LLM: `ai/qwen2.5:0.5B-F16`.

## Mise en place du dernier agent Milo pour la mise en forme des donn√©es

C'est la derni√®re fonction, `Summarize` qui va fournir au dernier agent (**Milo**) le contenu (fourni par **Bill** via la fonction `FetchContent`) √† traiter ainsi que les instructions √† appliquer pour la mise en forme du r√©sultat. Cette fois ci, le travail de l'agent est simple, puisqu'il effectue une compl√©tion de chat tout ce qu'il y a de plus classique:

```golang
func Summarize(instructions string, content []string) (string, error) {

	model := "ai/qwen3:latest"

	Milo, _ := robby.NewAgent(
		robby.WithDMRClient(
			context.Background(),
			"http://model-runner.docker.internal/engines/llama.cpp/v1/",
		),
		robby.WithParams(
			openai.ChatCompletionNewParams{
				Model: model,
				Messages: []openai.ChatCompletionMessageParamUnion{
					openai.SystemMessage(strings.Join(content, "\n")),
					openai.UserMessage(instructions),
				},
				Temperature: openai.Opt(0.0),
				TopP:        openai.Opt(0.3), // Lowering TopP to reduce randomness
				// NOTE: To limit hallucinations and obtain more reliable responses,
				// lower both the ‚Äútemperature‚Äù and ‚Äútop_p‚Äù parameters.
				// This forces the model to choose the safest and most predictable answers.

			},
		),
	)
	result, err := Milo.ChatCompletionStream(func(self *robby.Agent, content string, err error) error {

		fmt.Print(content)
		return nil
	})
	if err != nil {
		return "", fmt.Errorf("error in ChatCompletionStream: %w", err)
	}
	fmt.Println("\nReport Generated ‚úÖ")
	return result, nil
}
```

Et nous obtiendrons un rapport markdown de ce type:

```markdown
# Summary of Docker Compose

Docker Compose is a tool for defining and running multi-container applications using a YAML file. It simplifies the management of services, networks, and volumes in a single configuration file. Below is a structured summary of the key information provided from the given sources.

---

## üîß Key Concepts in Docker Compose

- **Docker Compose File (YAML Format)**: The main configuration file is `docker-compose.yml`, which defines services, networks, and volumes.
  - **Version**: Specifies the format of the Compose file.
  - **Services**: Each service represents a containerized application component.
  - **Networks**: Custom networks for communication between containers.
  - **Volumes**: For data persistence and sharing between containers.

- **Services**: Each service runs a single container and can be configured with options like image, environment variables, and resource limits.

- **Networks**: Custom networks allow services to communicate with each other.

- **Volumes**: Used for data persistence and sharing between containers.

---

## üìÅ Docker Compose File Example

```yaml
version: '3.8'

services:
  web:
    image: nginx:latest
    ports:
      - "80:80"
    networks:
      - frontend
    volumes:
      - shared-volume:/usr/share/nginx/html
    depends_on:
      - app

  app:
    image: node:14
    working_dir: /app
    command: node server.js
    networks:
      - frontend
    volumes:
      - shared-volume:/app/data

networks:
  frontend:
    driver: bridge

volumes:
  shared-volume:
```

---

## üöÄ Why Use Docker Compose?

- Simplifies the management of multi-container applications.
- Provides a single YAML file for defining services, networks, and volumes.
- Supports the entire lifecycle of an application (start, stop, rebuild, etc.).
- Works in all environments (development, testing, production, CI/CD).

---

## üìö Docker Compose Commands

- `docker compose up`: Start services defined in the `docker-compose.yml` file.
- `docker compose down`: Stop and remove containers, networks, and volumes.
- `docker compose build`: Build services defined in the `docker-compose.yml` file.
- `docker compose logs`: View the logs of running services.
- `docker compose ps`: List containers started by Compose.

---

## üìö References

- [Docker Compose Documentation](https://docs.docker.com/compose/)
- [Docker Compose on GeeksforGeeks](https://www.geeksforgeeks.org/docker-compose/)
- [GitHub Repository for Docker Compose](https://github.com/docker/compose)

---

## üìå Best Practices

- Use a single `docker-compose.yml` file for managing all services.
- Define custom networks for better isolation and communication.
- Use volumes for data persistence and sharing.
- Keep the Compose file version up to date for compatibility.

---

## üìå Features

- Multi-container application support.
- YAML-based configuration.
- Integration with Docker Swarm.
- One-off commands for running tasks on services.
- Lifecycle management commands.

---

## üìå Installation

- **Windows and macOS**: Included in Docker Desktop.
- **Linux**: Download binaries from the [GitHub release page](https://github.com/docker/compose/releases) and install them.

---

## üìå Conclusion

Docker Compose is a powerful tool for managing multi-container applications. It simplifies the setup and management of services, networks, and volumes, making it an essential part of modern DevOps workflows
```

Voil√†, maintenant, √† partir de 4 fonctions r√©utilisables, je dispose d'un syst√®me de recherche sur le web am√©lior√©e. Bien s√ªr c'est tr√®s largement am√©liorable, mais l'objectif √©tait de montrer qu'il √©tait finalement assez simple de fabriquer des applications d'IA g√©n√©ratives en d√©coupant et distribuant les responsabilit√©s √† plusieurs agents.
